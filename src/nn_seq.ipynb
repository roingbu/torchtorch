{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d,MaxPool2d,Flatten,Linear\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset =torchvision.datasets.CIFAR10(\"../data\", train=False, \n",
    "                                      transform=torchvision.transforms.ToTensor(),\n",
    "                                      download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class seqseq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(seqseq, self).__init__()\n",
    "        # self.conv1 = Conv2d(3, 32, 5,padding=2)  # dilation 默认值是1, 即不进行空洞卷积\n",
    "        # self.maxpool1 = MaxPool2d(2)\n",
    "        # self.conv2 = Conv2d(32,32,5,padding=2)\n",
    "        # self.maxpool2 = MaxPool2d(2)\n",
    "        # self.conv3 = Conv2d(32,64,5,padding=2)\n",
    "        # self.maxpool3 = MaxPool2d(2)\n",
    "        # self.flatten = Flatten()\n",
    "        # self.linear1 = Linear(1024,64)\n",
    "        # self.linear2 = Linear(64,10)\n",
    "        \n",
    "        self.model1 = nn.Sequential(\n",
    "            Conv2d(3, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 64, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Flatten(),\n",
    "            Linear(1024, 64),\n",
    "            Linear(64, 10)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = self.conv1(x)\n",
    "        # x = self.maxpool1(x)\n",
    "        # x = self.conv2(x)\n",
    "        # x = self.maxpool2(x)\n",
    "        # x = self.conv3(x)\n",
    "        # x = self.maxpool3(x)\n",
    "        # x = self.flatten(x)\n",
    "        # x = self.linear1(x)\n",
    "        # x = self.linear2(x)\n",
    "        \n",
    "        x = self.model1(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqseq(\n",
      "  (model1): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=1024, out_features=64, bias=True)\n",
      "    (8): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bunn = seqseq()\n",
    "print(bunn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16157.4424, grad_fn=<AddBackward0>)\n",
      "tensor(15406.6641, grad_fn=<AddBackward0>)\n",
      "tensor(16027.9688, grad_fn=<AddBackward0>)\n",
      "tensor(17792.8516, grad_fn=<AddBackward0>)\n",
      "tensor(19788.5957, grad_fn=<AddBackward0>)\n",
      "tensor(22079.9609, grad_fn=<AddBackward0>)\n",
      "tensor(23882.6152, grad_fn=<AddBackward0>)\n",
      "tensor(24432.3203, grad_fn=<AddBackward0>)\n",
      "tensor(25283.4043, grad_fn=<AddBackward0>)\n",
      "tensor(25742.2852, grad_fn=<AddBackward0>)\n",
      "tensor(26111.8672, grad_fn=<AddBackward0>)\n",
      "tensor(26727.9199, grad_fn=<AddBackward0>)\n",
      "tensor(28205.5234, grad_fn=<AddBackward0>)\n",
      "tensor(28667.1309, grad_fn=<AddBackward0>)\n",
      "tensor(30291.7793, grad_fn=<AddBackward0>)\n",
      "tensor(nan, grad_fn=<AddBackward0>)\n",
      "tensor(nan, grad_fn=<AddBackward0>)\n",
      "tensor(nan, grad_fn=<AddBackward0>)\n",
      "tensor(nan, grad_fn=<AddBackward0>)\n",
      "tensor(nan, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_cross = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(bunn.parameters(),lr=0.01)\n",
    "\n",
    "for epoch in range(20):\n",
    "    running_loss = 0.0\n",
    "    for data in dataloader:\n",
    "        imgs, targets = data\n",
    "        outputs = bunn(imgs)\n",
    "        # print(outputs)\n",
    "        # print(targets)\n",
    "        result_loss = loss_cross(outputs, targets)\n",
    "        # print(result_loss)\n",
    "        optim.zero_grad()  # 把网络中之前的梯度清零\n",
    "        result_loss.backward()  # 载入新的梯度\n",
    "        optim.step()  # 根据新的梯度更新权重\n",
    "        # print(result_loss)\n",
    "        running_loss = running_loss + result_loss   # 单个epoch的loss总和\n",
    "    print(running_loss.item()) # item()作用是把tensor值转换为数字\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.ones((64,3,32,32))\n",
    "\n",
    "\n",
    "output = bunn(input)\n",
    "print(output.shape)  # torch.size([64,10])  64是batchsize，10是10维\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "writer = SummaryWriter(\"../logs_bunn\")\n",
    "writer.add_graph(bunn,input)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn the reshape\n",
    "x = torch.tensor([0.1, 0.2, 0.3])\n",
    "print(x.shape)\n",
    "x\n",
    "x = torch.reshape(x, (1, 3))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# leaning customize model\n",
    "\n",
    "vgg16_false = torchvision.models.vgg16(weights=None)\n",
    "\n",
    "# print(vgg16_false)\n",
    "# vgg16_false.add_module(\"add_linear\", nn.Linear(1000, 10))  # classifier外面加模块\n",
    "# print(vgg16_false)\n",
    "\n",
    "# vgg16_false.classifier.add_module(\"add_linear\", nn.Linear(1000, 10))  # classifier里面加模块\n",
    "# print(vgg16_false)\n",
    "\n",
    "vgg16_false.classifier[6] = nn.Linear(4096,10)  # 直接内部修改\n",
    "print(vgg16_false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络保存\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型结构和参数\n",
    "# 如果是自定义的模型，则在加载的时候，前面要加该模型的类\n",
    "torch.save(vgg16_false, \"../models/vgg16_false.pth\")\n",
    "# 仅保存参数，且保存成字典  \n",
    "# 官方推荐\n",
    "torch.sava(vgg16_false.state_dict(),\"../models/vgg16_mothod2.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 对方法一的加载\n",
    "model = torch.load(\"../models/vgg16_false.pth\")\n",
    "print(model)\n",
    "\n",
    "# 注意, 如果是自定义模型\n",
    "\n",
    "class seqseq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(seqseq, self).__init__()\n",
    "        # self.conv1 = Conv2d(3, 32, 5,padding=2)  # dilation 默认值是1, 即不进行空洞卷积\n",
    "        # self.maxpool1 = MaxPool2d(2)\n",
    "        # self.conv2 = Conv2d(32,32,5,padding=2)\n",
    "        # self.maxpool2 = MaxPool2d(2)\n",
    "        # self.conv3 = Conv2d(32,64,5,padding=2)\n",
    "        # self.maxpool3 = MaxPool2d(2)\n",
    "        # self.flatten = Flatten()\n",
    "        # self.linear1 = Linear(1024,64)\n",
    "        # self.linear2 = Linear(64,10)\n",
    "        \n",
    "        self.model1 = nn.Sequential(\n",
    "            Conv2d(3, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 64, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Flatten(),\n",
    "            Linear(1024, 64),\n",
    "            Linear(64, 10)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = self.conv1(x)\n",
    "        # x = self.maxpool1(x)\n",
    "        # x = self.conv2(x)\n",
    "        # x = self.maxpool2(x)\n",
    "        # x = self.conv3(x)\n",
    "        # x = self.maxpool3(x)\n",
    "        # x = self.flatten(x)\n",
    "        # x = self.linear1(x)\n",
    "        # x = self.linear2(x)\n",
    "        \n",
    "        x = self.model1(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "model = torch.load(\"../models/bunn.pth\")  # 前面不需要初始化模型\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 对方法二的加载\n",
    "vgg16_mothod2 = torchvision.models.vgg16(weights=None)\n",
    "vgg16_mothod2.load_state_dict(torch.load(\"../models/vgg16_mothod2.pth\"))\n",
    "# model = torch.load(\"../models/vgg16_mothod2.pth\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7962f67a52dc5143011d2ddbb364fd3c87141f5018263783e87428a089dc07aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
